{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7794936-94d2-4570-b24f-0745dd1d79f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ab3791d-9854-4057-aac8-aac20aa6cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "\n",
    "conf.set(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1\")\n",
    "\n",
    "conf.set('loglevel', 'ERROR')\n",
    "\n",
    "SparkContext(conf=conf)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredNetworkStreaming\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d6bd2f-6a4d-498c-ad82-ac71de1d1527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "394c8018-a921-4331-ac2f-6904bab585eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.streaming import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6868f27b-8928-48cd-9b6a-71eaad6221e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://0e69f7f92047:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb71db9a260>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f88a196c-d4f6-430d-8f5c-fdf87bce4d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Missing application resource.\n",
      "\n",
      "Usage: spark-submit [options] <app jar | python file | R file> [app arguments]\n",
      "Usage: spark-submit --kill [submission ID] --master [spark://...]\n",
      "Usage: spark-submit --status [submission ID] --master [spark://...]\n",
      "Usage: spark-submit run-example [options] example-class [example args]\n",
      "\n",
      "Options:\n",
      "  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,\n",
      "                              k8s://https://host:port, or local (Default: local[*]).\n",
      "  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (\"client\") or\n",
      "                              on one of the worker machines inside the cluster (\"cluster\")\n",
      "                              (Default: client).\n",
      "  --class CLASS_NAME          Your application's main class (for Java / Scala apps).\n",
      "  --name NAME                 A name of your application.\n",
      "  --jars JARS                 Comma-separated list of jars to include on the driver\n",
      "                              and executor classpaths.\n",
      "  --packages                  Comma-separated list of maven coordinates of jars to include\n",
      "                              on the driver and executor classpaths. Will search the local\n",
      "                              maven repo, then maven central and any additional remote\n",
      "                              repositories given by --repositories. The format for the\n",
      "                              coordinates should be groupId:artifactId:version.\n",
      "  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while\n",
      "                              resolving the dependencies provided in --packages to avoid\n",
      "                              dependency conflicts.\n",
      "  --repositories              Comma-separated list of additional remote repositories to\n",
      "                              search for the maven coordinates given with --packages.\n",
      "  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place\n",
      "                              on the PYTHONPATH for Python apps.\n",
      "  --files FILES               Comma-separated list of files to be placed in the working\n",
      "                              directory of each executor. File paths of these files\n",
      "                              in executors can be accessed via SparkFiles.get(fileName).\n",
      "  --archives ARCHIVES         Comma-separated list of archives to be extracted into the\n",
      "                              working directory of each executor.\n",
      "\n",
      "  --conf, -c PROP=VALUE       Arbitrary Spark configuration property.\n",
      "  --properties-file FILE      Path to a file from which to load extra properties. If not\n",
      "                              specified, this will look for conf/spark-defaults.conf.\n",
      "\n",
      "  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n",
      "  --driver-java-options       Extra Java options to pass to the driver.\n",
      "  --driver-library-path       Extra library path entries to pass to the driver.\n",
      "  --driver-class-path         Extra class path entries to pass to the driver. Note that\n",
      "                              jars added with --jars are automatically included in the\n",
      "                              classpath.\n",
      "\n",
      "  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n",
      "\n",
      "  --proxy-user NAME           User to impersonate when submitting the application.\n",
      "                              This argument does not work with --principal / --keytab.\n",
      "\n",
      "  --help, -h                  Show this help message and exit.\n",
      "  --verbose, -v               Print additional debug output.\n",
      "  --version,                  Print the version of current Spark.\n",
      "\n",
      " Cluster deploy mode only:\n",
      "  --driver-cores NUM          Number of cores used by the driver, only in cluster mode\n",
      "                              (Default: 1).\n",
      "\n",
      " Spark standalone or Mesos with cluster deploy mode only:\n",
      "  --supervise                 If given, restarts the driver on failure.\n",
      "\n",
      " Spark standalone, Mesos or K8s with cluster deploy mode only:\n",
      "  --kill SUBMISSION_ID        If given, kills the driver specified.\n",
      "  --status SUBMISSION_ID      If given, requests the status of the driver specified.\n",
      "\n",
      " Spark standalone, Mesos and Kubernetes only:\n",
      "  --total-executor-cores NUM  Total cores for all executors.\n",
      "\n",
      " Spark standalone, YARN and Kubernetes only:\n",
      "  --executor-cores NUM        Number of cores used by each executor. (Default: 1 in\n",
      "                              YARN and K8S modes, or all available cores on the worker\n",
      "                              in standalone mode).\n",
      "\n",
      " Spark on YARN and Kubernetes only:\n",
      "  --num-executors NUM         Number of executors to launch (Default: 2).\n",
      "                              If dynamic allocation is enabled, the initial number of\n",
      "                              executors will be at least NUM.\n",
      "  --principal PRINCIPAL       Principal to be used to login to KDC.\n",
      "  --keytab KEYTAB             The full path to the file that contains the keytab for the\n",
      "                              principal specified above.\n",
      "\n",
      " Spark on YARN only:\n",
      "  --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "!spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1,org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0474011-1a6c-4bde-93b3-0198c079922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8881918a-aaaf-4773-a941-84b1817621b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaDF = spark\\\n",
    "        .readStream.format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\",\"kafka:9093\") \\\n",
    "        .option(\"subscribe\",\"velib-stations\") \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "730f60a4-5be8-4028-b27a-56d2d47e1037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafkaDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25c940a2-02cd-41c3-8f54-abf900e7d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#on crée un dataframe qui represente le flux en entrée de kafka\n",
    "\n",
    "KAFKA_SERVER = \"localhost:9093\"\n",
    "KAFKA_TOPIC = \"velib-stations\"\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"contract_name\", StringType(), True),\n",
    "    #StructField(\"last_update\", TimestampType(), False),\n",
    "    StructField(\"number\", IntegerType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"bike_stands\", IntegerType(), True),\n",
    "])\n",
    "    \n",
    "kafkaDF = spark\\\n",
    "        .readStream.format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\",\"kafka:9092\") \\\n",
    "        .option(\"subscribe\",\"velib-stations\") \\\n",
    "        .load()\\\n",
    "        .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\")) \\\n",
    "        .select(col(\"parsed_value.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1ba74d4-bee3-45f7-ab40-549041f0bdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(kafkaDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0515d487-61be-4fb4-8716-a647f6db5f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contract_name: string (nullable = true)\n",
      " |-- number: integer (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- bike_stands: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafkaDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0515c17-f549-48d5-8e11-b04786154013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw = spark.sql(\"select * from qraw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423af509-64e2-43a9-9995-ec575ccf31b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = kafkaDF.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882ff814-16ab-42e3-9ac0-2e5b4f8b2b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578e4455-9113-44af-afc7-8d4b07ef027a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
